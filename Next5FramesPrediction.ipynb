{"cells":[{"cell_type":"code","execution_count":1,"id":"0894ac1b-854e-4c08-a52d-c7a77e5fa40a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19921,"status":"ok","timestamp":1738483869875,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"},"user_tz":-300},"id":"0894ac1b-854e-4c08-a52d-c7a77e5fa40a","outputId":"03521ce9-a6cf-4064-a7af-53ded6619c98","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikeras\n","  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n","Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.0)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n","Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n","Installing collected packages: scikeras\n","Successfully installed scikeras-0.13.0\n"]}],"source":["# !pip install split-folders\n","!pip install scikeras\n","\n","#import mediapipe as mp\n","import time\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import cv2\n","import tensorflow as tf\n","import math\n","from tensorflow.keras.layers import Bidirectional, LSTM, Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Reshape, GlobalAveragePooling1D, Add, Reshape\n","from tensorflow.keras.models import Model, Sequential\n","from sklearn.metrics import mean_absolute_error\n","from keras.layers import Input, MultiHeadAttention, Dense, Dropout, LayerNormalization, Add, Reshape, Flatten\n","\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import LSTM, Dense, Reshape, Bidirectional, Input, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Dropout, Add, Flatten\n","from tensorflow.keras.utils import plot_model\n","import itertools\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers import LSTM, Bidirectional, Dense, Reshape\n","from sklearn.metrics import mean_absolute_error\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_absolute_error\n","from scikeras.wrappers import KerasRegressor\n","\n","from collections import Counter\n","from tensorflow.keras.models import load_model,Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Reshape\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn; sn.set(font_scale=1.4)\n","from sklearn.utils import shuffle\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":2,"id":"51bf9d7a-7a2d-4336-8927-f7681d3d3c88","metadata":{"id":"51bf9d7a-7a2d-4336-8927-f7681d3d3c88","scrolled":true,"executionInfo":{"status":"ok","timestamp":1738483869878,"user_tz":-300,"elapsed":20,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":["# import splitfolders\n","# splitfolders.ratio(\"D:\\\\Stroke Data\", output=\"D:\\\\Stroke Data_Splitted\\\\\",seed=1337, ratio=(.7, .3), group_prefix=None, move=False)"]},{"cell_type":"code","execution_count":3,"id":"2d-uAxVFxayn","metadata":{"id":"2d-uAxVFxayn","executionInfo":{"status":"ok","timestamp":1738483869879,"user_tz":-300,"elapsed":18,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":["'''\n","def draw_and_save_model(model, filename):\n","    # Plot the model architecture and save as an image\n","    plot_model(model, to_file=filename, show_shapes=True, show_layer_names=True)\n","    img = plt.imread(filename)\n","    plt.figure(figsize=(10, 10))\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.show()\n","'''\n","# Function to plot and save BERT model architecture\n","def draw_and_save_model(model, filename):\n","    # Plot the model architecture with increased DPI for better image quality and readability\n","    plot_model(model, to_file=filename, show_shapes=True, show_layer_names=True,\n","               dpi=150)  # Increased DPI for better resolution\n","    img = plt.imread(filename)\n","    plt.figure(figsize=(15, 15))  # Larger figure for better visibility\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.show()\n","\n","def lstm_model(NextFrames,data,X_train, y_train, X_test, y_test,train_size, outputunit, learning_rate, batch_size):\n","  # Build the LSTM model\n","  model = Sequential()\n","  model.add(LSTM(outputunit, activation='relu', return_sequences=True, input_shape=(sequence_length, data.shape[1])))\n","  #model.add(LSTM(128, activation='relu', return_sequences=True, input_shape=(sequence_length, data.shape[1])))\n","  model.add(LSTM(outputunit, activation='relu'))\n","  #model.add(LSTM(128, activation='relu'))\n","  model.add(Dense(NextFrames * data.shape[1]))  # Predicting 3 frames (3 * 132 features)\n","  model.add(Reshape((NextFrames, data.shape[1])))  # Reshape output to (3, 132)\n","  # Compile the model\n","    # Compile with adjustable learning rate\n","  optimizer = Adam(learning_rate=learning_rate)\n","  model.compile(optimizer=optimizer, loss='mean_squared_error')\n","  #optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","  #model.compile(optimizer=optimizer, loss='mse')\n","  #model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","  # Train the model\n","  model.fit(X_train, y_train, epochs=50, batch_size=batch_size, validation_split=0.2, verbose=0)\n","  # Predict the next 3 frames\n","  print(f\"X_test[:1].shape {X_test[:1].shape}\")\n","  predictions_scaled = model.predict(X_test[:1])  # Predict only the first set of test data (next 3 frames)\n","  # Inverse transform the predictions to the original scale\n","  predictions = scaler.inverse_transform(predictions_scaled.reshape(NextFrames, -1))\n","  # Get the actual next 5 frames (841, 842, 843)\n","  actual = data[train_size:train_size+NextFrames]\n","  #print(f\"y_test shape: {y_test.shape}, actual.shape: {actual.shape}\")\n","  # Evaluate performance using Mean Absolute Error (MAE)\n","  mae = mean_absolute_error(actual, predictions)\n","  return model,actual,predictions, mae\n","\n","def Bilstm_model(NextFrames,data,X_train, y_train, X_test, y_test,train_size, outputunit, learning_rate, batch_size):\n","  #sequence_length = X_train.shape[1]  # Define based on your data\n","  #n_features = X_train.shape[2]       # Define based on your data\n","  model = Sequential()\n","  model.add(Bidirectional(LSTM(outputunit, activation='relu', return_sequences=True), input_shape=(sequence_length, data.shape[1])))\n","  #model.add(Bidirectional(LSTM(128, activation='relu', return_sequences=True), input_shape=(sequence_length, data.shape[1])))\n","  model.add(Bidirectional(LSTM(outputunit, activation='relu')))\n","  #model.add(Bidirectional(LSTM(128, activation='relu')))\n","  model.add(Dense(NextFrames * data.shape[1] ))  # Predicting next frames with shape (NextFrames * n_features)\n","  model.add(Reshape((NextFrames, data.shape[1])))  # Reshape to (NextFrames, n_features)\n","\n","  optimizer = Adam(learning_rate=learning_rate)\n","  model.compile(optimizer=optimizer, loss='mean_squared_error')\n","  #optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","  #model.compile(optimizer=optimizer, loss='mse')\n","\n","  #model.compile(optimizer='adam', loss='mse')\n","\n","  # Train the model\n","  model.fit(X_train, y_train, epochs=50, batch_size=batch_size, validation_split=0.2, verbose=0)\n","  # Predict the next 3 frames\n","  predictions_scaled = model.predict(X_test[:1])  # Predict only the first set of test data (next 3 frames)\n","\n","  # Inverse transform the predictions to the original scale\n","  predictions = scaler.inverse_transform(predictions_scaled.reshape(NextFrames, -1))\n","\n","  # Get the actual next 3 frames (841, 842, 843)\n","  actual = data[train_size:train_size+NextFrames]\n","\n","  # Evaluate performance using Mean Absolute Error (MAE)\n","  mae = mean_absolute_error(actual, predictions)\n","  return model, actual,predictions, mae\n","\n","################################\n","\n","def transformer_model(NextFrames, sequence_length, n_features, X_train, y_train, X_test, y_test,train_size, outputunit, MultiHead,dropout):\n","    input_layer = Input(shape=(sequence_length, n_features))\n","\n","    # Multi-Head Attention Layer\n","    attention_output = MultiHeadAttention(num_heads=MultiHead, key_dim=n_features)(input_layer, input_layer)\n","                     #MultiHeadAttention(num_heads=4, key_dim=n_features)(input_layer, input_layer)\n","    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n","\n","    # Fully Connected Layers\n","    transformer_output = Dense(outputunit, activation='relu')(attention_output)\n","                       #Dense(64, activation='relu')(attention_output)\n","    transformer_output = Dropout(dropout)(transformer_output)\n","                        #Dropout(0.2)(transformer_output)\n","\n","    # Pooling to reduce dimensions to match NextFrames\n","    transformer_output = GlobalAveragePooling1D()(transformer_output)\n","    # Predict the next frames (NextFrames * n_features)\n","    output_layer = Dense(NextFrames * n_features)(transformer_output)\n","    output_layer = Reshape((NextFrames, n_features))(output_layer)\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    model.compile(optimizer='adam', loss='mse')\n","    # Train the model\n","    model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n","    # Predict for the entire X_test set\n","    #predictions = model.predict(X_test)\n","      # Predict the next 3 frames\n","    print(f\"X_test[:1].shape {X_test[:1].shape}\")\n","\n","    predictions_scaled = model.predict(X_test[:1])  # Predict only the first set of test data (next 3 frames)\n","    predictions = scaler.inverse_transform(predictions_scaled.reshape(NextFrames, -1))\n","\n","    actual = data[train_size:train_size+NextFrames]\n","    print(f\"actual.shape = {actual.shape}, prediction.shape = {predictions.shape}\")\n","    #ytest=y_test.reshape(-1, n_features)\n","    #predic=predictions.reshape(-1, n_features)\n","\n","    #print(f\"y_test.reshape(-1, n_features)= {ytest.shape}, predictions.reshape(-1, n_features) = {predic.shape}, actual shape: {actual.shape}\")\n","\n","    # Evaluate performance using Mean Absolute Error (MAE)\n","    #mae = mean_absolute_error(y_test.reshape(-1, n_features), predictions.reshape(-1, n_features))\n","    mae = mean_absolute_error(actual, predictions)\n","    print(f\"ourput unit: {outputunit} dropout= {dropout}, MultiHead = {MultiHead}, mae: {mae}\")\n","    return model, actual, predictions, mae\n","\n","###########################################\n","def bert_model(NextFrames, sequence_length, n_features, X_train, y_train, X_test, y_test, train_size, MultiHead, dropout, num_transformer_blocks):\n","    input_layer = Input(shape=(sequence_length, n_features))\n","\n","    # Transformer blocks emulating BERT layers\n","    x = input_layer\n","    for _ in range(num_transformer_blocks):\n","        # Multi-Head Attention layer with normalization and dropout\n","        attention_output = MultiHeadAttention(num_heads=MultiHead, key_dim=n_features)(x, x)\n","                          #MultiHeadAttention(num_heads=4, key_dim=n_features)(x, x)\n","        attention_output = Dropout(dropout)(attention_output)\n","        attention_output = Add()([x, attention_output])  # Residual connection\n","        attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n","\n","        # Feed-forward network\n","        ffn_output = Dense(n_features, activation='relu')(attention_output)  # Match n_features to maintain shape\n","        ffn_output = Dropout(dropout)(ffn_output)  #Dropout(0.1)(ffn_output)\n","        x = Add()([attention_output, ffn_output])  # Another residual connection\n","        x = LayerNormalization(epsilon=1e-6)(x)\n","\n","    # Flatten and add dense layer to ensure correct output shape\n","    x = Flatten()(x)\n","    output_layer = Dense(NextFrames * n_features)(x)\n","    output_layer = Reshape((NextFrames, n_features))(output_layer)\n","\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    model.compile(optimizer='adam', loss='mse')\n","\n","    # Train the model\n","    model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n","\n","    # Predict for the entire X_test set\n","    predictions_scaled = model.predict(X_test[:1])  # Predict only the first set of test data (next frames)\n","\n","    # Assuming `scaler` and `data` are defined, adapt as needed\n","    predictions = scaler.inverse_transform(predictions_scaled.reshape(NextFrames, -1))\n","    actual = data[train_size:train_size + NextFrames]\n","\n","    mae = mean_absolute_error(actual, predictions)\n","    print(f\"MultiHead: {MultiHead}, dropout:{dropout},  MAE: {mae}\")\n","    return model, actual, predictions, mae\n"]},{"cell_type":"code","execution_count":3,"id":"q2YjtXzjxbUY","metadata":{"id":"q2YjtXzjxbUY","executionInfo":{"status":"ok","timestamp":1738483878779,"user_tz":-300,"elapsed":899,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"id":"jTgfu_DrB2QS","metadata":{"id":"jTgfu_DrB2QS","executionInfo":{"status":"ok","timestamp":1738483879731,"user_tz":-300,"elapsed":4,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":["def model_evaluation(NextFrames, y_test,y_pred, ModelName, stroke, predictedFrameNumber):\n","\n","  from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","  # Initialize lists to store metrics for each frame\n","  mae_scores = []\n","  mse_scores = []\n","  r2_scores = []\n","\n","  # Loop through each of the 5 frames\n","  for i in range(NextFrames):\n","      # Get the actual and predicted values for the i-th frame\n","      actual_frame = y_test[ i, :]    # Shape: (num_samples, n_features)\n","      predicted_frame = y_pred[ i, :] # Shape: (num_samples, n_features)\n","\n","      # Calculate MAE, MSE, and R² for the i-th frame\n","      mae = mean_absolute_error(actual_frame, predicted_frame)\n","      mse = mean_squared_error(actual_frame, predicted_frame)\n","      r2 = r2_score(actual_frame, predicted_frame)\n","\n","      # Append the scores to the lists\n","      mae_scores.append(mae)\n","      mse_scores.append(mse)\n","      r2_scores.append(r2)\n","\n","      #plt.subplot(3, 1, i+1)\n","      #plt.plot(, label='Predicted Frame {}'.format(train_size + i + 1))\n","      predictions_rescaled=predicted_frame\n","      test_data_scaled = actual_frame\n","      # Print predicted vs actual frames for comparison\n","      print(\"Predicted frames:\")\n","      #print(predictions_rescaled)\n","      print(\"Actual frames:\")\n","      #print(test_data_scaled)\n","      pred_actual_frame(Posdataset, predictions_rescaled, test_data_scaled)\n","\n","      preframes = predictions_rescaled\n","      # Reshape frames to 2D (if it is a 3D array)\n","      preframes = preframes.reshape(1, 132)\n","      dataset = Posdataset.drop(columns=['Unnamed: 0', 'Label'], axis=1)\n","      # Assuming 'newdataset' is your DataFrame\n","      columns_list = dataset.columns.tolist()\n","\n","      # Now create the DataFrame\n","      PredictedFrame = pd.DataFrame(preframes, columns=columns_list)\n","\n","      orgframes = test_data_scaled\n","      orgframes = orgframes.reshape(1, 132)\n","      OrignalFrame = pd.DataFrame(orgframes, columns=columns_list)\n","\n","      #VisualEvaluation_3Actual_Predicted_Frame(PredictedFrame, OrignalFrame)\n","      predictedFrameNumber=predictedFrameNumber+i\n","      mse,mse,r2=qualitative_quantitative_diff(PredictedFrame, OrignalFrame, ModelName, stroke,predictedFrameNumber)\n","      # Display the scores for each frame\n","  print(f\" { ModelName } Model Evaluation for Batsman's Stroke {stroke}: \")\n","  return mse,mse,r2, ModelName, stroke\n","  '''\n","  for i in range(NextFrames):\n","      print(f\"Frame {i+1}: MAE = {mae_scores[i]}, MSE = {mse_scores[i]}, R² = {r2_scores[i]}\")\n","  # Average scores across all frames\n","  average_mae = sum(mae_scores) / NextFrames\n","  average_mse = sum(mse_scores) / NextFrames\n","  average_r2 = sum(r2_scores) / NextFrames\n","\n","  print(f\"Over All {ModelName} Average MAE: {average_mae}\")\n","  print(f\"Over All {ModelName}  Average MSE: {average_mse}\")\n","  print(f\"Over All {ModelName}  Average R²: {average_r2}\")\n","  '''\n","#############################################\n","\n","import pandas as pd\n","import numpy as np\n","import plotly.graph_objects as go\n","\n","def extract_coordinates(df):\n","    joints = 33  # Number of joints\n","    x, y, z = [], [], []\n","\n","    for i in range(joints):\n","        x.append(df[f'F{i}_x'].values[0])  # Take the first row's x value\n","        y.append(df[f'F{i}_y'].values[0])  # Take the first row's y value\n","        z.append(df[f'F{i}_z'].values[0])  # Take the first row's z value\n","\n","    return np.array(x), np.array(y), np.array(z)\n","\n","# Function to plot the skeleton for a single pose in 3D\n","def plot_skeleton(x, y, z, connections, color='blue'):\n","    lines = []\n","    for joint1, joint2 in connections:\n","        lines.append(go.Scatter3d(\n","            x=[x[joint1], x[joint2]],\n","            y=[y[joint1], y[joint2]],\n","            z=[z[joint1], z[joint2]],\n","            mode='lines',\n","            line=dict(color=color, width=5)\n","        ))\n","    return lines\n","\n","def VisualEvaluation_3Actual_Predicted_Frame(PredictedFrame, OrignalFrame):\n","  # Load DataFrame with 33 joints (columns: 'x', 'y', 'z', 'visibility')\n","  df1 = PredictedFrame  # Replace with your actual data\n","  df2 = OrignalFrame  # Replace with your actual data\n","\n","  # Extract coordinates for both poses\n","  x1, y1, z1 = extract_coordinates(df1)\n","  x2, y2, z2 = extract_coordinates(df2)\n","\n","  # Define connections between joints based on MediaPipe PoseNet model\n","  POSE_CONNECTIONS = [\n","      (0, 1), (1, 2), (2, 3), (3, 7),  # Right arm\n","      (0, 4), (4, 5), (5, 6), (6, 8),  # Left arm\n","      (9, 10), (11, 12),               # Shoulders\n","      (11, 13), (13, 15), (15, 17),    # Left leg\n","      (12, 14), (14, 16), (16, 18),    # Right leg\n","      (11, 23), (12, 24),              # Hips to shoulders\n","      (23, 24), (23, 25), (24, 26),    # Hips and upper legs\n","      (25, 27), (27, 29), (29, 31),    # Left lower leg\n","      (26, 28), (28, 30), (30, 32)     # Right lower leg\n","  ]\n","  '''\n","  # Create the 3D plot with both poses\n","  fig = go.Figure()\n","\n","  # Add skeleton for both poses (in red and blue)\n","  fig.add_traces(plot_skeleton(x1, y1, z1, POSE_CONNECTIONS, color='red'))\n","  fig.add_traces(plot_skeleton(x2, y2, z2, POSE_CONNECTIONS, color='blue'))\n","\n","  # Set the layout for the 3D plot with increased size\n","  fig.update_layout(\n","      title='3D Pose Skeleton Visualization',\n","      scene=dict(\n","          xaxis=dict(title='X-axis'),\n","          yaxis=dict(title='Y-axis'),\n","          zaxis=dict(title='Z-axis', range=[-1, 1]),  # Adjust range as needed\n","          aspectmode='cube'\n","      ),\n","      width=1200,  # Increased width\n","      height=800,  # Increased height\n","      showlegend=False\n","  )\n","\n","  # Display the plot\n","  fig.show()\n","  '''\n","#####################################################################\n","\n","def qualitative_quantitative_diff(predictions_rescaled, y_test_rescaled, model_name, stroke, predictedFrameNumber):\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","  # Assuming predictions_rescaled and y_test_rescaled are numpy arrays\n","  # Reshape the arrays to be one-dimensional for better comparison\n","  predictions_rescaled_flat = np.ravel(predictions_rescaled)\n","  y_test_rescaled_flat = np.ravel(y_test_rescaled)\n","\n","  # Ensure the two arrays are of the same length by truncating if necessary\n","  min_length = min(len(predictions_rescaled_flat), len(y_test_rescaled_flat))\n","  predictions_rescaled_flat = predictions_rescaled_flat[:min_length]\n","  y_test_rescaled_flat = y_test_rescaled_flat[:min_length]\n","\n","  # Define the x-axis labels\n","  x_labels = [\"Nose_x\", \"Nose_y\", \"Nose_z\", \" \", \"LEyeInner_x\", \"LEyeInner_y\", \"LEyeInner_z\", \" \",\n","              \"LEye_x\", \"LEye_y\", \"LEye_z\", \" \", \"LEyeOuter_x\", \"LEyeOuter_y\", \"LEyeOuter_z\", \" \",\n","              \"REyeInner_x\", \"REyeInner_y\", \"REyeInner_z\", \" \", \"REye_x\", \"REye_y\", \"REye_z\", \" \",\n","              \"REyeOuter_x\", \"REyeOuter_y\", \"REyeOuter_z\", \" \", \"LEar_x\", \"LEar_y\", \"LEar_z\", \" \",\n","              \"REar_x\", \"REar_y\", \"REar_z\", \" \", \"LMouth_x\", \"LMouth_y\", \"LMouth_z\", \" \",\n","              \"RMouth_x\", \"RMouth_y\", \"RMouth_z\", \" \", \"LShoulder_x\", \"LShoulder_y\", \"LShoulder_z\", \" \",\n","              \"RShoulder_x\", \"RShoulder_y\", \"RShoulder_z\", \" \", \"LElbow_x\", \"LElbow_y\", \"LElbow_z\", \" \",\n","              \"RElbow_x\", \"RElbow_y\", \"RElbow_z\", \" \", \"LWrist_x\", \"LWrist_y\", \"LWrist_z\", \" \",\n","              \"RWrist_x\", \"RWrist_y\", \"RWrist_z\", \" \", \"LPinky_x\", \"LPinky_y\", \"LPinky_z\", \" \",\n","              \"RPinky_x\", \"RPinky_y\", \"RPinky_z\", \" \",    \"LIndex_x\", \"LIndex_y\", \"LIndex_z\", \" \",\n","              \"RIndex_x\", \"RIndex_y\", \"RIndex_z\", \" \",    \"LThumb_x\", \"LThumb _y\", \"LThumb _z\", \" \",\n","              \"RThumb_x\", \"RThumb _y\", \"RThumb _z\", \" \",    \"LHip_x\", \"LHip_y\", \"LHip_z\", \" \",\n","              \"RHip_x\", \"RHip_y\", \"RHip_z\", \" \", \"LKnee_x\", \"LKnee_y\", \"LKnee_z\", \" \",\n","              \"RKnee_x\", \"RKnee_y\", \"RKnee_z\", \" \", \"LAnkle_x\", \"LAnkle_y\", \"LAnkle_z\", \" \",\n","              \"RAnkle_x\", \"RAnkle_y\", \"RAnkle_z\", \" \", \"LHeel_x\", \"LHeel_y\", \"LHeel_z\", \" \",\n","              \"RHeel_x\", \"RHeel_y\", \"RHeel_z\", \" \",\"LFootIdx_x\", \"LFootIdx_y\", \"LFootIdx_z\", \" \",\n","              \"RFootIdx_x\", \"RFootIdx_y\", \"RFootIdx_z\", \" \",\n","           ]\n","\n","\n","  # Quantitative metrics\n","  mse = mean_squared_error(y_test_rescaled_flat, predictions_rescaled_flat)\n","  mae = mean_absolute_error(y_test_rescaled_flat, predictions_rescaled_flat)\n","  r2 = r2_score(y_test_rescaled_flat, predictions_rescaled_flat)\n","  print(f\"Mean Squared Error (MSE): {mse}\")\n","  print(f\"Mean Absolute Error (MAE): {mae}\")\n","  print(f\"R-squared (R²): {r2}\")\n","\n","  # Visual comparison: Plot actual vs. predicted values\n","  plt.figure(figsize=(15, 8))\n","  plt.plot(y_test_rescaled_flat, label='Actual Values', color='blue', marker='o')\n","  plt.plot(predictions_rescaled_flat, label='Predicted Values', color='red',  linestyle='--', marker='x')\n","  plt.title('Comparison of Actual and ' + model_name + '-Predicted Body Points at Frame '+ str(predictedFrameNumber) +' for ' + stroke + ' Motion')\n","\n","  plt.xlabel('Data Points')\n","  plt.xticks(ticks=np.arange(len(x_labels)), labels=x_labels, rotation=90)\n","  plt.ylabel('Rescaled Values')\n","  plt.legend()\n","  plt.grid(True)\n","  plt.tight_layout()\n","  plt.show()\n","\n","  return mse,mse,r2\n","\n","###############################\n","\n","def All_qualitative_quantitative_diff0(\n","        actual, LSTMpredictions, BiLSTMpredictions, TransformerPredictions, BERTpredictions, stroke):\n","    import numpy as np\n","    import matplotlib.pyplot as plt\n","\n","    # Flatten predictions and actual values to ensure they are one-dimensional arrays\n","    LSTMpredictions_flat = np.ravel(LSTMpredictions)\n","    BiLSTMpredictions_flat = np.ravel(BiLSTMpredictions)\n","    TransformerPredictions_flat = np.ravel(TransformerPredictions)\n","    BERTpredictions_flat = np.ravel(BERTpredictions)\n","    actual_flat = np.ravel(actual)\n","\n","    # Define the y-axis labels (132 keypoints as given in the original code)\n","    y_labels = [\n","        \"Nose_x\", \"Nose_y\", \"Nose_z\", \" \", \"LEyeInner_x\", \"LEyeInner_y\", \"LEyeInner_z\", \" \",\n","        \"LEye_x\", \"LEye_y\", \"LEye_z\", \" \", \"LEyeOuter_x\", \"LEyeOuter_y\", \"LEyeOuter_z\", \" \",\n","        \"REyeInner_x\", \"REyeInner_y\", \"REyeInner_z\", \" \", \"REye_x\", \"REye_y\", \"REye_z\", \" \",\n","        \"REyeOuter_x\", \"REyeOuter_y\", \"REyeOuter_z\", \" \", \"LEar_x\", \"LEar_y\", \"LEar_z\", \" \",\n","        \"REar_x\", \"REar_y\", \"REar_z\", \" \", \"LMouth_x\", \"LMouth_y\", \"LMouth_z\", \" \",\n","        \"RMouth_x\", \"RMouth_y\", \"RMouth_z\", \" \", \"LShoulder_x\", \"LShoulder_y\", \"LShoulder_z\", \" \",\n","        \"RShoulder_x\", \"RShoulder_y\", \"RShoulder_z\", \" \", \"LElbow_x\", \"LElbow_y\", \"LElbow_z\", \" \",\n","        \"RElbow_x\", \"RElbow_y\", \"RElbow_z\", \" \", \"LWrist_x\", \"LWrist_y\", \"LWrist_z\", \" \",\n","        \"RWrist_x\", \"RWrist_y\", \"RWrist_z\", \" \", \"LPinky_x\", \"LPinky_y\", \"LPinky_z\", \" \",\n","        \"RPinky_x\", \"RPinky_y\", \"RPinky_z\", \" \", \"LIndex_x\", \"LIndex_y\", \"LIndex_z\", \" \",\n","        \"RIndex_x\", \"RIndex_y\", \"RIndex_z\", \" \", \"LThumb_x\", \"LThumb_y\", \"LThumb_z\", \" \",\n","        \"RThumb_x\", \"RThumb_y\", \"RThumb_z\", \" \", \"LHip_x\", \"LHip_y\", \"LHip_z\", \" \",\n","        \"RHip_x\", \"RHip_y\", \"RHip_z\", \" \", \"LKnee_x\", \"LKnee_y\", \"LKnee_z\", \" \",\n","        \"RKnee_x\", \"RKnee_y\", \"RKnee_z\", \" \", \"LAnkle_x\", \"LAnkle_y\", \"LAnkle_z\", \" \",\n","        \"RAnkle_x\", \"RAnkle_y\", \"RAnkle_z\", \" \", \"LHeel_x\", \"LHeel_y\", \"LHeel_z\", \" \",\n","        \"RHeel_x\", \"RHeel_y\", \"RHeel_z\", \" \", \"LFootIdx_x\", \"LFootIdx_y\", \"LFootIdx_z\", \" \",\n","        \"RFootIdx_x\", \"RFootIdx_y\", \"RFootIdx_z\", \" \"\n","    ]\n","\n","    # Plot the actual values and predictions in a horizontal layout\n","    plt.figure(figsize=(12, 16))\n","    plt.plot(actual_flat, range(len(y_labels)), label='Actual', color='blue', marker='o', markersize=6, linewidth=2, alpha=0.8)\n","    plt.plot(LSTMpredictions_flat, range(len(y_labels)), label='LSTM', color='red', linestyle='--', marker='x', markersize=5, linewidth=1.5, alpha=0.7)\n","    plt.plot(BiLSTMpredictions_flat, range(len(y_labels)), label='BiLSTM', color='green', linestyle='-.', marker='s', markersize=5, linewidth=1.5, alpha=0.7)\n","    plt.plot(TransformerPredictions_flat, range(len(y_labels)), label='Transformer', color='purple', linestyle=':', marker='^', markersize=5, linewidth=1.5, alpha=0.7)\n","    plt.plot(BERTpredictions_flat, range(len(y_labels)), label='BERT', color='orange', linestyle='-', marker='d', markersize=5, linewidth=1.5, alpha=0.7)\n","\n","    plt.title(f'Comparison of Actual and Predicted Body Points for {stroke} Motion', fontsize=16, weight='bold')\n","    plt.ylabel('Body Keypoints', fontsize=12)\n","    plt.yticks(ticks=np.arange(len(y_labels)), labels=y_labels, fontsize=8)\n","    plt.xlabel('Values', fontsize=12)\n","    plt.legend(fontsize=10, loc='upper right')\n","    plt.grid(True, linestyle='--', alpha=0.5)\n","    plt.tight_layout()\n","    plt.show()\n","\n","##################################################\n","\n","def All_qualitative_quantitative_diff(\n","        actual, LSTMpredictions, BiLSTMpredictions, TransformerPredictions, BERTpredictions, stroke):\n","    import numpy as np\n","    import matplotlib.pyplot as plt\n","\n","    # Flatten predictions and actual values to ensure they are one-dimensional arrays\n","    LSTMpredictions_flat = np.ravel(LSTMpredictions)\n","    BiLSTMpredictions_flat = np.ravel(BiLSTMpredictions)\n","    TransformerPredictions_flat = np.ravel(TransformerPredictions)\n","    BERTpredictions_flat = np.ravel(BERTpredictions)\n","    actual_flat = np.ravel(actual)\n","\n","    # Define the x-axis labels (132 keypoints as given in the original code)\n","    x_labels = [\n","        \"Nose_x\", \"Nose_y\", \"Nose_z\", \" \", \"LEyeInner_x\", \"LEyeInner_y\", \"LEyeInner_z\", \" \",\n","        \"LEye_x\", \"LEye_y\", \"LEye_z\", \" \", \"LEyeOuter_x\", \"LEyeOuter_y\", \"LEyeOuter_z\", \" \",\n","        \"REyeInner_x\", \"REyeInner_y\", \"REyeInner_z\", \" \", \"REye_x\", \"REye_y\", \"REye_z\", \" \",\n","        \"REyeOuter_x\", \"REyeOuter_y\", \"REyeOuter_z\", \" \", \"LEar_x\", \"LEar_y\", \"LEar_z\", \" \",\n","        \"REar_x\", \"REar_y\", \"REar_z\", \" \", \"LMouth_x\", \"LMouth_y\", \"LMouth_z\", \" \",\n","        \"RMouth_x\", \"RMouth_y\", \"RMouth_z\", \" \", \"LShoulder_x\", \"LShoulder_y\", \"LShoulder_z\", \" \",\n","        \"RShoulder_x\", \"RShoulder_y\", \"RShoulder_z\", \" \", \"LElbow_x\", \"LElbow_y\", \"LElbow_z\", \" \",\n","        \"RElbow_x\", \"RElbow_y\", \"RElbow_z\", \" \", \"LWrist_x\", \"LWrist_y\", \"LWrist_z\", \" \",\n","        \"RWrist_x\", \"RWrist_y\", \"RWrist_z\", \" \", \"LPinky_x\", \"LPinky_y\", \"LPinky_z\", \" \",\n","        \"RPinky_x\", \"RPinky_y\", \"RPinky_z\", \" \", \"LIndex_x\", \"LIndex_y\", \"LIndex_z\", \" \",\n","        \"RIndex_x\", \"RIndex_y\", \"RIndex_z\", \" \", \"LThumb_x\", \"LThumb_y\", \"LThumb_z\", \" \",\n","        \"RThumb_x\", \"RThumb_y\", \"RThumb_z\", \" \", \"LHip_x\", \"LHip_y\", \"LHip_z\", \" \",\n","        \"RHip_x\", \"RHip_y\", \"RHip_z\", \" \", \"LKnee_x\", \"LKnee_y\", \"LKnee_z\", \" \",\n","        \"RKnee_x\", \"RKnee_y\", \"RKnee_z\", \" \", \"LAnkle_x\", \"LAnkle_y\", \"LAnkle_z\", \" \",\n","        \"RAnkle_x\", \"RAnkle_y\", \"RAnkle_z\", \" \", \"LHeel_x\", \"LHeel_y\", \"LHeel_z\", \" \",\n","        \"RHeel_x\", \"RHeel_y\", \"RHeel_z\", \" \", \"LFootIdx_x\", \"LFootIdx_y\", \"LFootIdx_z\", \" \",\n","        \"RFootIdx_x\", \"RFootIdx_y\", \"RFootIdx_z\", \" \"\n","    ]\n","\n","    # Plot the actual values and predictions\n","    plt.figure(figsize=(18, 10))\n","    plt.plot(actual_flat, label='Actual', color='blue', marker='o', markersize=6, linewidth=2, alpha=0.8)\n","    plt.plot(LSTMpredictions_flat, label='LSTM', color='red', linestyle='--', marker='x', markersize=5, linewidth=1.5, alpha=0.7)\n","    plt.plot(BiLSTMpredictions_flat, label='BiLSTM', color='green', linestyle='-.', marker='s', markersize=5, linewidth=1.5, alpha=0.7)\n","    plt.plot(TransformerPredictions_flat, label='Transformer', color='purple', linestyle=':', marker='^', markersize=5, linewidth=1.5, alpha=0.7)\n","    plt.plot(BERTpredictions_flat, label='BERT', color='orange', linestyle='-', marker='d', markersize=5, linewidth=1.5, alpha=0.7)\n","\n","    plt.title(f'Comparison of Actual and Predicted Body Points for {stroke} Motion', fontsize=16, weight='bold')\n","    plt.xlabel('Body Keypoints', fontsize=12)\n","    plt.xticks(ticks=np.arange(len(x_labels)), labels=x_labels,rotation=90,  fontsize=13)\n","\n","    plt.ylabel('Values', fontsize=12)\n","    plt.legend(fontsize=10)\n","    plt.grid(True, linestyle='--', alpha=0.5)\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":4,"id":"WMbL91dPY1HE","metadata":{"id":"WMbL91dPY1HE","executionInfo":{"status":"ok","timestamp":1738483889002,"user_tz":-300,"elapsed":1375,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"id":"MSwB7Q9057Yg","metadata":{"id":"MSwB7Q9057Yg","executionInfo":{"status":"ok","timestamp":1738483890177,"user_tz":-300,"elapsed":4,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","def pred_actual_frame( Posdataset, predictions_rescaled, y_test_rescaled):\n","    #Posdataset\n","    import cv2\n","    import pandas as pd\n","    import numpy as np\n","    dataset = Posdataset.drop(columns=['Unnamed: 0', 'Label'], axis=1)\n","    # Assuming 'newdataset' is your DataFrame\n","    columns_list = dataset.columns.tolist()\n","\n","    preframes=predictions_rescaled\n","\n","    # Reshape frames to 2D (if it is a 3D array)\n","    preframes = preframes.reshape(1, 132)\n","\n","    # Now create the DataFrame\n","    PredictedFrame = pd.DataFrame(preframes, columns=columns_list)\n","\n","    orgframes = y_test_rescaled\n","    orgframes = orgframes.reshape(1, 132)\n","    OrignalFrame = pd.DataFrame(orgframes, columns=columns_list)\n","\n","\n","    # Select only the columns that contain x, y, and z coordinates\n","    xyz_columns = [col for col in PredictedFrame.columns if '_x' in col or '_y' in col or '_z' in col]\n","    predicted_xyz = PredictedFrame[xyz_columns]\n","    original_xyz = OrignalFrame[xyz_columns]\n","\n","    # Define connections between keypoints to draw the skeleton based on F1 to F32\n","    connections = [\n","        (0, 1), (1, 2), (2, 3),  # Left eye connections\n","        (0, 4), (4, 5), (5, 6),  # Right eye connections\n","        (0, 7), (0, 8),  # Nose to ears\n","        (9, 10),  # Mouth connection\n","        (11, 12),  # Shoulders\n","        (11, 13), (13, 15),  # Left arm (shoulder, elbow, wrist)\n","        (15, 17), (15, 19), (15, 21),  # Left hand (wrist to pinky, index, thumb)\n","        (12, 14), (14, 16),  # Right arm (shoulder, elbow, wrist)\n","        (16, 18), (16, 20), (16, 22),  # Right hand (wrist to pinky, index, thumb)\n","        (23, 24),  # Hips\n","        (11, 23), (12, 24),  # Shoulders to hips\n","        (23, 25), (25, 27), (27, 29), (27, 31),  # Left leg (hip, knee, ankle, heel, foot index)\n","        (24, 26), (26, 28), (28, 30), (28, 32),  # Right leg (hip, knee, ankle, heel, foot index)\n","    ]\n","\n","    # Create a blank image (adjust size according to your coordinates)\n","    width, height = 640, 480  # Adjust size as needed\n","\n","    # Loop through each row (frame) in the DataFrame to draw the skeleton for each frame\n","    for index in range(len(predicted_xyz)):\n","        # Clear the image for each frame\n","        blank_image = np.zeros((height, width, 3), dtype=np.uint8)\n","\n","        # Extract (x, y) coordinates from the predicted row\n","        predicted_row = predicted_xyz.iloc[index]\n","        predicted_keypoints = []\n","        for i in range(0, len(predicted_row), 3):  # Skip every 3rd value because of the x, y, z format\n","            x = predicted_row[i] * width  # Assuming x is normalized, scale to image width\n","            y = predicted_row[i + 1] * height  # Assuming y is normalized, scale to image height\n","            predicted_keypoints.append((int(x), int(y)))\n","\n","        # Extract (x, y) coordinates from the original row\n","        original_row = original_xyz.iloc[index]\n","        original_keypoints = []\n","        for i in range(0, len(original_row), 3):  # Skip every 3rd value because of the x, y, z format\n","            x = original_row[i] * width  # Assuming x is normalized, scale to image width\n","            y = original_row[i + 1] * height  # Assuming y is normalized, scale to image height\n","            original_keypoints.append((int(x), int(y)))\n","\n","        # Draw circles at each keypoint for the predicted frame (color: green)\n","        for (x, y) in predicted_keypoints:\n","            cv2.circle(blank_image, (x, y), 5, (0, 255, 0), cv2.FILLED)\n","\n","        # Draw lines to form the skeleton for the predicted frame (color: blue)\n","        for connection in connections:\n","            point1 = connection[0]\n","            point2 = connection[1]\n","            if point1 < len(predicted_keypoints) and point2 < len(predicted_keypoints):\n","                cv2.line(blank_image, predicted_keypoints[point1], predicted_keypoints[point2], (0, 255, 0), 2)\n","\n","        # Draw circles at each keypoint for the original frame (color: red)\n","        for (x, y) in original_keypoints:\n","            cv2.circle(blank_image, (x, y), 5, (0, 0, 255), cv2.FILLED)\n","\n","        # Draw lines to form the skeleton for the original frame (color: yellow)\n","        for connection in connections:\n","            point1 = connection[0]\n","            point2 = connection[1]\n","            if point1 < len(original_keypoints) and point2 < len(original_keypoints):\n","                cv2.line(blank_image, original_keypoints[point1], original_keypoints[point2], (0, 0, 255), 2)\n","\n","        # Add text to indicate which color represents original and predicted\n","        cv2.putText(blank_image, 'Original: Pose Red', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n","        cv2.putText(blank_image, 'Predicted: Pose Green', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n","        #cv2.putText(blank_image, 'Original: Shot ' + str(Org_label[0]), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n","        #cv2.putText(blank_image, 'Predicted: Shot ' + str(pred_label[0]), (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n","\n","        # Show the skeleton for this frame\n","        #cv2.imshow('Skeleton Comparison', blank_image)\n","        '''\n","        cv2_imshow( blank_image)\n","        key = cv2.waitKey(50000) & 0xFF  # Adjust wait time to 30 milliseconds\n","        if key == ord('q'):\n","            print(\"Exiting on 'q' key press\")\n","            break\n","        '''\n","    # Release resources\n","    cv2.destroyAllWindows()\n","\n","\n","    # Select only the columns that contain x, y, and z coordinates\n","    xyz_columns = [col for col in PredictedFrame.columns if '_x' in col or '_y' in col or '_z' in col]\n","    predicted_xyz = PredictedFrame[xyz_columns]\n","    original_xyz = OrignalFrame[xyz_columns]\n","\n","    # Define connections between keypoints to draw the skeleton based on F1 to F32\n","    connections = [\n","        (0, 1), (1, 2), (2, 3),  # Left eye connections\n","        (0, 4), (4, 5), (5, 6),  # Right eye connections\n","        (0, 7), (0, 8),  # Nose to ears\n","        (9, 10),  # Mouth connection\n","        (11, 12),  # Shoulders\n","        (11, 13), (13, 15),  # Left arm (shoulder, elbow, wrist)\n","        (15, 17), (15, 19), (15, 21),  # Left hand (wrist to pinky, index, thumb)\n","        (12, 14), (14, 16),  # Right arm (shoulder, elbow, wrist)\n","        (16, 18), (16, 20), (16, 22),  # Right hand (wrist to pinky, index, thumb)\n","        (23, 24),  # Hips\n","        (11, 23), (12, 24),  # Shoulders to hips\n","        (23, 25), (25, 27), (27, 29), (27, 31),  # Left leg (hip, knee, ankle, heel, foot index)\n","        (24, 26), (26, 28), (28, 30), (28, 32),  # Right leg (hip, knee, ankle, heel, foot index)\n","    ]\n","\n","    # Create a blank image (adjust size according to your coordinates)\n","    width, height = 640, 480  # Adjust size as needed\n","\n","    # Loop through each row (frame) in the DataFrame to draw the skeleton for each frame\n","    for index in range(len(predicted_xyz)):\n","        # Clear images for each frame\n","        blank_image_predicted = np.zeros((height, width, 3), dtype=np.uint8)\n","        blank_image_original = np.zeros((height, width, 3), dtype=np.uint8)\n","\n","        # Extract (x, y) coordinates from the predicted row\n","        predicted_row = predicted_xyz.iloc[index]\n","        predicted_keypoints = []\n","        for i in range(0, len(predicted_row), 3):  # Skip every 3rd value because of the x, y, z format\n","            x = predicted_row[i] * width  # Assuming x is normalized, scale to image width\n","            y = predicted_row[i + 1] * height  # Assuming y is normalized, scale to image height\n","            predicted_keypoints.append((int(x), int(y)))\n","\n","        # Extract (x, y) coordinates from the original row\n","        original_row = original_xyz.iloc[index]\n","        original_keypoints = []\n","        for i in range(0, len(original_row), 3):  # Skip every 3rd value because of the x, y, z format\n","            x = original_row[i] * width  # Assuming x is normalized, scale to image width\n","            y = original_row[i + 1] * height  # Assuming y is normalized, scale to image height\n","            original_keypoints.append((int(x), int(y)))\n","\n","        # Draw circles and lines for the predicted frame (color: green)\n","        for (x, y) in predicted_keypoints:\n","            cv2.circle(blank_image_predicted, (x, y), 5, (0, 255, 0), cv2.FILLED)\n","        for connection in connections:\n","            point1, point2 = connection\n","            if point1 < len(predicted_keypoints) and point2 < len(predicted_keypoints):\n","                cv2.line(blank_image_predicted, predicted_keypoints[point1], predicted_keypoints[point2], (0, 255, 0), 2)\n","\n","        # Draw circles and lines for the original frame (color: red)\n","        for (x, y) in original_keypoints:\n","            cv2.circle(blank_image_original, (x, y), 5, (0, 0, 255), cv2.FILLED)\n","        for connection in connections:\n","            point1, point2 = connection\n","            if point1 < len(original_keypoints) and point2 < len(original_keypoints):\n","                cv2.line(blank_image_original, original_keypoints[point1], original_keypoints[point2], (0, 0, 255), 2)\n","        '''\n","        # Add text for frame labels\n","        cv2.putText(blank_image_original, 'Original', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n","        cv2.putText(blank_image_predicted, 'Predicted', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n","\n","        # Concatenate the two images side by side\n","        comparison_image = np.hstack((blank_image_original, blank_image_predicted))\n","\n","        # Display the side-by-side comparison\n","        #cv2.imshow('Original vs Predicted Posture', comparison_image)\n","        cv2_imshow( comparison_image)\n","        # Wait for keypress or a delay between frames\n","        key = cv2.waitKey(50000) & 0xFF  # Adjust the delay as needed\n","        if key == ord('q'):\n","            print(\"Exiting on 'q' key press\")\n","            break\n","        '''\n","    # Release resources\n","    cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":5,"id":"NxtOQIIH2gFv","metadata":{"id":"NxtOQIIH2gFv","executionInfo":{"status":"ok","timestamp":1738483894588,"user_tz":-300,"elapsed":1723,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"id":"280223bb-6eee-4a37-92da-af8eb2d30f3d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"280223bb-6eee-4a37-92da-af8eb2d30f3d","scrolled":true,"outputId":"1233c973-9723-4985-8c05-3fc209aa63d2","executionInfo":{"status":"ok","timestamp":1738486141156,"user_tz":-300,"elapsed":986424,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","(850, 132)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739ms/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.05202040461794605\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615ms/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.05769045545163106\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 683ms/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.04962903547973045\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step\n","MultiHead: 4, dropout:0.2,  MAE: 0.06342145930357519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7891707fa7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.045992484788308585\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7891701b9760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 515ms/step\n","MultiHead: 8, dropout:0.2,  MAE: 0.053211181060432156\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.05932351475423093\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.06292410192559109\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.05177633062587257\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451ms/step\n","MultiHead: 4, dropout:0.2,  MAE: 0.05430858168411675\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.05013368856997833\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 545ms/step\n","MultiHead: 8, dropout:0.2,  MAE: 0.04796981750405517\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.04712866716476455\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.055023933411812394\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 522ms/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.05030511102442492\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step\n","MultiHead: 4, dropout:0.2,  MAE: 0.052812150880567774\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428ms/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.057834880059095865\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428ms/step\n","MultiHead: 8, dropout:0.2,  MAE: 0.054578085386000244\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.061749286389954305\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 789ms/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.09295000183731725\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850ms/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.0427503479368113\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n","MultiHead: 4, dropout:0.2,  MAE: 0.07550589112155444\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.053397584191318334\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785ms/step\n","MultiHead: 8, dropout:0.2,  MAE: 0.07479609296216971\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671ms/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.0696483823167421\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.06071268714338318\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670ms/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.04817660623403104\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 4, dropout:0.2,  MAE: 0.055231221425573365\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.05416573328478201\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n","MultiHead: 8, dropout:0.2,  MAE: 0.05755790292056264\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661ms/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.05821520500616015\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668ms/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.04665496611959178\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661ms/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.05685124873911748\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n","MultiHead: 4, dropout:0.2,  MAE: 0.07806503743383605\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.03906041853946101\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 797ms/step\n","MultiHead: 8, dropout:0.2,  MAE: 0.06763205173802236\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.051784200124394525\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.05329132784164926\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.03100855920052818\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 4, dropout:0.2,  MAE: 0.04553553528348338\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.08099474548866759\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 8, dropout:0.2,  MAE: 0.041202160160228735\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.051598532181585346\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.04553702337926483\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.0666515469182366\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 4, dropout:0.2,  MAE: 0.04558926775487117\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.03578206590942548\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 8, dropout:0.2,  MAE: 0.06358945654673696\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 2, dropout:0.1,  MAE: 0.05898084185153844\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 2, dropout:0.2,  MAE: 0.04607399509261789\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 4, dropout:0.1,  MAE: 0.06839145072811259\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 4, dropout:0.2,  MAE: 0.04419753237738636\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 8, dropout:0.1,  MAE: 0.03825156433587267\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n","MultiHead: 8, dropout:0.2,  MAE: 0.04367576911415075\n","Stroke: back_foot_punch, outputunit: 4, MultiHead: 8, num_transformer_blocks 4,  MAE: 0.04367576911415075\n","Grid search completed. Results saved to /content/drive/My Drive/Cricket_Enhancement/Drive/bert_grid_search_results.csv\n"]}],"source":["from glob import glob\n","from google.colab import drive\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Prepare the data for LSTM (create sequences)\n","def create_sequences(data, sequence_length, NextFrames):\n","\n","    xs, ys = [], []\n","    for i in range(len(data) - sequence_length - NextFrames):  # We want to predict next 3 frames\n","        x = data[i:i+sequence_length]\n","        y = data[i+sequence_length:i+sequence_length+NextFrames]  # Predict 3 future frames\n","        xs.append(x)\n","        ys.append(y)\n","    return np.array(xs), np.array(ys)\n","\n","#mpPose = mp.solutions.pose\n","#pose = mpPose.Pose()\n","#mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n","#points = mpPose.PoseLandmark # Landmarks\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","base_path = '/content/drive/My Drive/Cricket_Enhancement/Drive/'  # Adjust as needed\n","\n","\n","# Load your datasets\n","Straight_Drive_df = pd.read_csv(base_path + \"Straight_Drive.csv\")\n","Sweep_df = pd.read_csv(base_path + \"Sweep.csv\")\n","Pull_df = pd.read_csv(base_path + \"Pull.csv\")\n","onDrive_df = pd.read_csv(base_path + \"onDrive.csv\")\n","Flick_df = pd.read_csv(base_path + \"Flick.csv\")\n","Cut_df = pd.read_csv(base_path + \"Cut.csv\")\n","Cover_Drive_df = pd.read_csv(base_path + \"Cover_Drive.csv\")\n","back_foot_punch_df = pd.read_csv(base_path + \"back_foot_punch.csv\")\n","\n","# List of dataframes\n","dataframes = [\n","    #Straight_Drive_df,  Sweep_df, Pull_df, onDrive_df, Flick_df, Cut_df,   Cover_Drive_df,\n","    back_foot_punch_df\n","    ]\n","# Optional: List of names to identify each dataframe\n","names = [\n","    #\"Straight_Drive\", Sweep\",   \"Pull\", \"onDrive\",\"Flick\",  \"Cut\",     \"Cover_Drive\",\n","    \"back_foot_punch\"\n","    ]\n","#Now you can work with your DataFrames\n","#print(Cover_Drive_df.head())\n","results = []\n","\n","#results_path = base_path + \"original_BiLSTM_grid_search_results.csv\"\n","#results_path = base_path + \"original_Transformer_grid_search_results.csv\"\n","results_path = base_path + \"bert_grid_search_results.csv\"\n","# Initialize an empty DataFrame to store the results\n","data_to_save = []\n","for i, df in enumerate(dataframes):\n","  data = []\n","  a=0\n","  # Load the dataset (replace 'Cover_Drive.csv' with your actual dataset file)\n","  #Posdataset =  Cover_Drive_df #pd.read_csv('Cover_Drive.csv')\n","  #print(f\"\\nFirst 3 rows of {names[i]}:\")\n","  Posdataset=df\n","  stroke=names[i]\n","  # Drop unnecessary columns if they exist in each dataframe\n","  if 'Unnamed: 0' in df.columns and 'Label' in df.columns:\n","      Posdataset_numeric = df.drop(columns=['Unnamed: 0', 'Label'], axis=1)\n","  else:\n","      Posdataset_numeric = df  # If no unnecessary columns to drop\n","  # Extract all PoseNet columns (assuming columns are named as in your example)\n","  columns = ['F{}_{}'.format(i, axis) for i in range(33) for axis in ['x', 'y', 'z', 'vis']]\n","  data = Posdataset[columns].values\n","\n","  # Normalize the data\n","  scaler = MinMaxScaler()\n","  data_scaled = scaler.fit_transform(data)\n","  print(data.shape)\n","\n","  # Define a smaller sequence length (e.g., use 5 previous frames to predict the next 3 frames)\n","  sequence_length = 5 #10\n","  NextFrames=1 #5\n","  # Create sequences with the new sequence length\n","  #NextFrames=5 #3\n","  X, y = create_sequences(data_scaled, sequence_length, NextFrames)\n","  #print(math.floor(X.shape[0]*0.9) )\n","  predictedFrameNumber=math.floor(X.shape[0]*0.2)\n","  # Adjust the train/test split (e.g., 800 rows for training)\n","  train_size = math.floor(X.shape[0]*0.9) #800  # Adjust based on the number of frames you have\n","\n","  X_train, y_train = X[:train_size], y[:train_size]\n","  X_test, y_test = X[train_size:], y[train_size:]\n","\n","  from itertools import product\n","  import pandas as pd\n","\n","  # Define the hyperparameter grid\n","  '''\n","  param_grid_LSTM = {\n","      'outputunit': [32, 64, 128],\n","      'learning_rate': [0.1, 0.01, 0.001],\n","       'batch_size': [16, 32, 64],\n","      }\n","  grid_combinations = list(product(*param_grid_LSTM.values()))\n","  param_names = list(param_grid_LSTM.keys())\n","\n","  param_grid_BiLSTM = {\n","      'outputunit': [32, 64, 128],\n","      'learning_rate': [0.1,0.01, 0.001],\n","      'batch_size': [16, 32, 64],\n","      }\n","  grid_combinations = list(product(*param_grid_BiLSTM.values()))\n","  param_names = list(param_grid_BiLSTM.keys())\n","\n","  param_grid_Transformer = {      'outputunit':[32, 64,128],      'MultiHead': [2, 4, 8],      'dropout': [0.1,0.2],\n","\n","      }\n","  grid_combinations = list(product(*param_grid_Transformer.values()))\n","  param_names = list(param_grid_Transformer.keys())\n","  '''\n","  param_grid_BERT ={\n","      'num_transformer_blocks': [1,2,4],\n","      'outputunit': [1, 2, 4],\n","      'MultiHead': [2, 4, 8],\n","      'dropout': [0.1,0.2],\n","      }\n","  #Generate all parameter combinations\n","  grid_combinations = list(product(*param_grid_BERT.values()))\n","  param_names = list(param_grid_BERT.keys())\n","\n","  # List to store the results\n","  results = []\n","\n","  # Grid search\n","  for params in grid_combinations:\n","      param_dict = dict(zip(param_names, params))\n","      num_transformer_blocks = param_dict['num_transformer_blocks']\n","      outputunit = param_dict['outputunit']\n","      MultiHead = param_dict['MultiHead']\n","      dropout = param_dict['dropout']\n","\n","      # Create sequences with the specified sequence_length and NextFrames\n","      X, y = create_sequences(data_scaled, sequence_length, NextFrames)\n","      train_size = math.floor(X.shape[0] * 0.9)\n","      X_train, y_train = X[:train_size], y[:train_size]\n","      X_test, y_test = X[train_size:], y[train_size:]\n","      n_features = data.shape[1]\n","\n","      # Train and evaluate the BERT model\n","      #_,_,_, mae = lstm_model(NextFrames, data, X_train, y_train, X_test, y_test, train_size, outputunit, learning_rate, batch_size)\n","      #_,_,_,mae = Bilstm_model(NextFrames, data, X_train, y_train, X_test, y_test, train_size, outputunit, learning_rate, batch_size)\n","\n","      _, _, _, mae = bert_model( NextFrames, sequence_length, n_features, X_train, y_train, X_test, y_test, train_size,\n","                                MultiHead, dropout, num_transformer_blocks=num_transformer_blocks )\n","      #_,_,_,mae = transformer_model(NextFrames, sequence_length, n_features, X_train, y_train, X_test, y_test,train_size,\n","      #outputunit, MultiHead,dropout)\n","\n","      # Store the results\n","      result = {**param_dict, 'mae': mae}\n","      results.append(result)\n","\n","\n","  # Save results to a CSV file\n","  results_df = pd.DataFrame(results)\n","  #results_path = base_path + \"bert_grid_search_results.csv\"\n","  #results_path = base_path + \"origianl_transformer_grid_search_results.csv\"\n","  #results_df.to_csv(results_path, index=False)\n","  print(f\"Stroke: {stroke}, outputunit: {outputunit}, MultiHead: {MultiHead}, num_transformer_blocks {num_transformer_blocks},  MAE: {mae}\")\n","  results_df.to_csv(results_path, mode='a', header=False, index=False)\n","  print(f\"Grid search completed. Results saved to {results_path}\")\n","\n","\n","  '''\n","  # Print to check the shape of the data\n","  print(f\"NextFrames: {NextFrames}, sequence_length: {sequence_length}, data.shape: {data.shape}\")\n","  print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n","  print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n","  print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n","  # Print to check the shape of the data\n","  print(f\"data.shape: {data.shape}\")\n","  print(f\"data.shape[0]: {data.shape[0]}, data.shape[1]: {data.shape[1]}, data.shape[2]: {data.shape[1]}\")\n","  '''\n","  # Ensure there are sequences in the test set\n","  if X_test.shape[0] == 0:\n","      raise ValueError(\"Test set (X_test) is empty. Adjust the data split or sequence length.\")\n","  '''\n","  # LSTM Model\n","  lstm, actual, LSTMpredictions, mae = lstm_model(NextFrames, data, X_train, y_train, X_test, y_test, train_size)\n","  #draw_and_save_model(lstm, 'lstm_model.png')\n","\n","  ModelName=\"LSTM\"\n","  #mse, mse, r2, ModelName, stroke = model_evaluation(NextFrames, actual, LSTMpredictions, \"LSTM\", stroke, predictedFrameNumber)\n","  data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Actual\", \"Values\": actual.tolist()})\n","  data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Predicted\", \"Values\": LSTMpredictions.tolist()})\n","  print(f'LSTM model Mean Absolute Error for next {NextFrames} frames: {mae}')\n","\n","  # BiLSTM Model\n","  ModelName=\"BiLSTM\"\n","  bilstm, actual, BiLSTMpredictions, mae = Bilstm_model(NextFrames, data, X_train, y_train, X_test, y_test, train_size)\n","  #draw_and_save_model(bilstm, 'bilstm_model.png')\n","\n","  #mse, mse, r2, ModelName, stroke = model_evaluation(NextFrames, actual, BiLSTMpredictions, \"BiLSTM\", stroke, predictedFrameNumber)\n","  #data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Actual\", \"Values\": actual.tolist()})\n","  data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Predicted\", \"Values\": BiLSTMpredictions.tolist()})\n","  print(f'BiLSTM model Mean Absolute Error for next {NextFrames} frames: {mae}')\n","\n","  #LSTM_GridSearch(NextFrames, data, X_train, y_train, X_test, y_test, train_size)\n","\n","  #BiLSTM_GridSearch(NextFrames, data, X_train, y_train, X_test, y_test, train_size)\n","\n","  # Transformer Model\n","  ModelName=\"Transformer\"\n","  n_features = data.shape[1]\n","  results_path = base_path + \"transformer_grid_search_results3.csv\"\n","  best_params, best_mae = Transformer_GridSearch(NextFrames, sequence_length, n_features, X_train, y_train, X_test, y_test, train_size, results_path)\n","\n","  print(f\"Best Parameters: {best_params}\")\n","  print(f\"Best MAE: {best_mae}\")\n","\n","\n","  #draw_and_save_model(bertModel, 'bert_model_improved.png')\n","  #mse, mse, r2, ModelName, stroke = model_evaluation(NextFrames, actual, BERTpredictions, \"BERT\", stroke, predictedFrameNumber)\n","  #data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Actual\", \"Values\": actual.tolist()})\n","  data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Predicted\", \"Values\": BERTpredictions.tolist()})\n","  print(f'BERT model Mean Absolute Error for next {NextFrames} frames: {mae}')\n"," '''"]},{"cell_type":"code","execution_count":9,"id":"u0FxUlBqaQZe","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":775,"status":"error","timestamp":1738486330316,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"},"user_tz":-300},"id":"u0FxUlBqaQZe","outputId":"feafe33b-b0ae-4b9f-9015-c9ddf254cd06"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"transformer_model() missing 3 required positional arguments: 'outputunit', 'MultiHead', and 'dropout'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-720696b4f9be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformermodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerPredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNextFrames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#draw_and_save_model(transformermodel, 'transformer_model.png')mae: 0.017732816221360977\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#mse, mse, r2, ModelName, stroke = model_evaluation(NextFrames, actual, TransformerPredictions, \"Transformer\", stroke, predictedFrameNumber)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: transformer_model() missing 3 required positional arguments: 'outputunit', 'MultiHead', and 'dropout'"]}],"source":["  transformermodel, actual, TransformerPredictions, mae = transformer_model(NextFrames, sequence_length, n_features, X_train, y_train, X_test, y_test, train_size)\n","\n","  #draw_and_save_model(transformermodel, 'transformer_model.png')mae: 0.017732816221360977\n","\n","  #mse, mse, r2, ModelName, stroke = model_evaluation(NextFrames, actual, TransformerPredictions, \"Transformer\", stroke, predictedFrameNumber)\n","  data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Actual\", \"Values\": actual.tolist()})\n","  data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Predicted\", \"Values\": TransformerPredictions.tolist()})\n","  print(f'Transformer model Mean Absolute Error for next {NextFrames} frames: {mae}')\n","\n","  # BERT Model\n","  ModelName=\"BERT\"\n","  n_features = data.shape[1]\n","  bertModel, actual, BERTpredictions, mae = bert_model(NextFrames, sequence_length, n_features, X_train, y_train, X_test, y_test, train_size)\n","\n","  #draw_and_save_model(bertModel, 'bert_model_improved.png')\n","  #mse, mse, r2, ModelName, stroke = model_evaluation(NextFrames, actual, BERTpredictions, \"BERT\", stroke, predictedFrameNumber)\n","  #data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Actual\", \"Values\": actual.tolist()})\n","  data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Predicted\", \"Values\": BERTpredictions.tolist()})\n","  print(f'BERT model Mean Absolute Error for next {NextFrames} frames: {mae}')\n","  All_qualitative_quantitative_diff(actual,LSTMpredictions, BiLSTMpredictions,TransformerPredictions,BERTpredictions, stroke)\n","\n","  # Save to CSV\n","  print(\"Results saved to 'model_predictions.csv' \")\n","  output_path = base_path + \"model_predictions.csv\"\n","  output_df = pd.DataFrame(data_to_save)\n","  output_df.to_csv(output_path, index=False)\n","\n","  print(f\"Results saved to: {output_path}\")\n","\n","  results_df = pd.DataFrame(results)\n","  print(results_df)\n"]},{"cell_type":"code","execution_count":null,"id":"e-sDhtdfQmIf","metadata":{"id":"e-sDhtdfQmIf","executionInfo":{"status":"aborted","timestamp":1738486141162,"user_tz":-300,"elapsed":8,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"METEyCMcA-Sq","metadata":{"id":"METEyCMcA-Sq","executionInfo":{"status":"aborted","timestamp":1738486141163,"user_tz":-300,"elapsed":9,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"id":"xks4x40qcz83","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"elapsed":917,"status":"error","timestamp":1738486341291,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"},"user_tz":-300},"id":"xks4x40qcz83","outputId":"ab7987fb-4b89-4a2b-c7e0-5687c2f4b37f"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'output_df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-d1651c1192dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"model_predictions.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Results saved to: {output_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'output_df' is not defined"]}],"source":["output_path = base_path + \"model_predictions.csv\"\n","output_df.to_csv(output_path, index=False)\n","print(f\"Results saved to: {output_path}\")"]},{"cell_type":"code","execution_count":null,"id":"0cLP9lqln6p-","metadata":{"id":"0cLP9lqln6p-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"E-M-vhuon6tT","metadata":{"id":"E-M-vhuon6tT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"id":"D_SQEILSd3ff","metadata":{"id":"D_SQEILSd3ff","executionInfo":{"status":"ok","timestamp":1738486346526,"user_tz":-300,"elapsed":717,"user":{"displayName":"Muhammad Amjad Raza","userId":"08372128113470176651"}}},"outputs":[],"source":["from scikeras.wrappers import KerasRegressor\n","from sklearn.model_selection import GridSearchCV\n","import numpy as np\n","\n","# Wrap the Transformer model\n","def create_transformer_model(sequence_length, n_features, num_heads=4, key_dim=64, dense_units=64, dropout_rate=0.2):\n","    input_layer = Input(shape=(sequence_length, n_features))\n","\n","    # Multi-Head Attention Layer\n","    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(input_layer, input_layer)\n","    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n","\n","    # Fully Connected Layers\n","    transformer_output = Dense(dense_units, activation='relu')(attention_output)\n","    transformer_output = Dropout(dropout_rate)(transformer_output)\n","\n","    # Pooling to reduce dimensions to match NextFrames\n","    transformer_output = GlobalAveragePooling1D()(transformer_output)\n","\n","    # Predict the next frames\n","    output_layer = Dense(NextFrames * n_features)(transformer_output)\n","    output_layer = Reshape((NextFrames, n_features))(output_layer)\n","\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    model.compile(optimizer='adam', loss='mse')\n","    return model\n","\n","def Transformer_GridSearch(NextFrames, sequence_length, n_features, X_train, y_train, X_test, y_test, train_size, results_path):\n","    from itertools import product\n","    from sklearn.metrics import mean_absolute_error\n","    import pandas as pd\n","\n","    # Define hyperparameter grid\n","    '''\n","    param_grid = {\n","        'num_heads': [2, 4, 8],\n","        'key_dim': [32, 64, 128],\n","        'dense_units': [32, 64, 128],\n","        'dropout_rate': [0.1, 0.2, 0.3],\n","        'batch_size': [16, 32, 64],\n","        'epochs': [10, 20, 30]\n","    }\n","    '''\n","        # Define hyperparameter grid\n","    param_grid = {\n","        'num_heads': [ 4, 6],\n","        'key_dim': [32, 64 ],\n","        'dense_units': [32, 64],\n","        'dropout_rate': [ 0.2, 0.3],\n","        'batch_size': [16, 32, 64],\n","        'epochs': [ 30]\n","    }\n","\n","    best_mae = float('inf')\n","    best_params = None\n","    results = []  # List to store all combinations and their MAE\n","\n","    # Iterate through all combinations of parameters\n","    for num_heads, key_dim, dense_units, dropout_rate, batch_size, epochs in product(\n","        param_grid['num_heads'],\n","        param_grid['key_dim'],\n","        param_grid['dense_units'],\n","        param_grid['dropout_rate'],\n","        param_grid['batch_size'],\n","        param_grid['epochs']\n","    ):\n","        print(f\"Testing params: num_heads={num_heads}, key_dim={key_dim}, dense_units={dense_units}, \"\n","              f\"dropout_rate={dropout_rate}, batch_size={batch_size}, epochs={epochs}\")\n","\n","        # Create the model with the current parameters\n","        model = create_transformer_model(sequence_length, n_features, num_heads, key_dim, dense_units, dropout_rate)\n","\n","        # Train the model\n","        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n","\n","        # Evaluate on the test set\n","        predictions = model.predict(X_test)\n","        mae = mean_absolute_error(y_test.reshape(-1, n_features), predictions.reshape(-1, n_features))\n","\n","        # Append the result to the list\n","        results.append({\n","            'num_heads': num_heads,\n","            'key_dim': key_dim,\n","            'dense_units': dense_units,\n","            'dropout_rate': dropout_rate,\n","            'batch_size': batch_size,\n","            'epochs': epochs,\n","            'MAE': mae\n","        })\n","\n","        # Track the best parameters\n","        if mae < best_mae:\n","            best_mae = mae\n","            best_params = {\n","                'num_heads': num_heads,\n","                'key_dim': key_dim,\n","                'dense_units': dense_units,\n","                'dropout_rate': dropout_rate,\n","                'batch_size': batch_size,\n","                'epochs': epochs\n","            }\n","        print(f'dense_units: {dense_units}, dropout_rate: {dropout_rate}, batch_size: {batch_size}, MAE: {mae}')\n","    # Save the results to a CSV file\n","    results_df = pd.DataFrame(results)\n","    results_df.to_csv(results_path, index=False)\n","    print(f\"Grid search results saved to: {results_path}\")\n","\n","    return best_params, best_mae\n"]},{"cell_type":"code","execution_count":null,"id":"2D5XSETod3Ja","metadata":{"id":"2D5XSETod3Ja"},"outputs":[],"source":["def LSTM_GridSearch(NextFrames, data, X_train, y_train, X_test, y_test, train_size):\n","\n","  # Define the hyperparameter grid\n","  param_grid = {\n","      'lstm_units': [64, 128, 256],\n","      'learning_rate': [0.001, 0.01],\n","      'batch_size': [16, 32,64],\n","      'epochs': [50, 100, 150]\n","  }\n","\n","  # Generate all combinations of hyperparameters\n","  grid_combinations = list(itertools.product(*param_grid.values()))\n","\n","  # Placeholder for results\n","  results = []\n","\n","  # Perform grid search\n","  for idx, (lstm_units, learning_rate, batch_size, epochs) in enumerate(grid_combinations):\n","      print(f\"Testing combination {idx + 1}/{len(grid_combinations)}: units={lstm_units}, lr={learning_rate}, batch={batch_size}, epochs={epochs}\")\n","\n","      lstm, actual, LSTMpredictions, mae =lstm_modelgrid(NextFrames, data, X_train, y_train, X_test, y_test, train_size, sequence_length, data.shape[1], lstm_units, learning_rate, batch_size, epochs)\n","\n","      results.append({'lstm_units': lstm_units, 'learning_rate': learning_rate, 'batch_size': batch_size, 'epochs': epochs, 'mae': mae})\n","      ModelName=\"LSTM\"\n","      #mse, mse, r2, ModelName, stroke = model_evaluation(NextFrames, actual, LSTMpredictions, \"LSTM\", stroke, predictedFrameNumber)\n","      #data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Actual\", \"Values\": actual.tolist()})\n","      data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Predicted\", \"Values\": LSTMpredictions.tolist()})\n","      print(f'LSTM model Mean Absolute Error for next {NextFrames} frames: {mae}')\n","      # Convert results to a DataFrame\n","      results_df = pd.DataFrame(results)\n","\n","\n","  csv_path = base_path + 'grid_search_results.csv'\n","\n","  print(f\"CSV file saved at: {csv_path}\")\n","\n","  results_df.to_csv(csv_path, index=False)\n","\n","  # Display the best configuration\n","  best_result = results_df.loc[results_df['mae'].idxmin()]\n","  print(\"\\nBest Configuration:\")\n","  print(best_result)\n","\n","  # Plot results\n","  plt.figure(figsize=(12, 6))\n","  plt.plot(range(len(results)), results_df['mae'], marker='o', label='MAE')\n","  plt.xticks(range(len(results)), [f\"{i+1}\" for i in range(len(results))], rotation=90)\n","  plt.xlabel('Parameter Combination Index')\n","  plt.ylabel('Mean Absolute Error (MAE)')\n","  plt.title('Grid Search Performance')\n","  plt.legend()\n","  plt.grid()\n","  plt.show()\n","\n","#######################################\n","def BiLSTM_GridSearch(NextFrames, data, X_train, y_train, X_test, y_test, train_size):\n","  # Hyperparameter grid\n","  param_grid = {\n","      'units': [64, 128, 256],\n","      'batch_size': [16, 32, 64],\n","      'epochs': [50, 100, 150],\n","      'learning_rate': [0.01, 0.001, 0.0005]\n","  }\n","\n","  # Generate all combinations of hyperparameters\n","  param_combinations = list(itertools.product(*param_grid.values()))\n","\n","  # Track results\n","  results = []\n","\n","  for params in param_combinations:\n","      units, batch_size, epochs, learning_rate = params\n","      mae = build_and_train_BiLSTM(units, batch_size, epochs, learning_rate, NextFrames, data, X_train, y_train, X_test, y_test, train_size)\n","      results.append({\n","          'units': units,\n","          'learning_rate': learning_rate,\n","          'batch_size': batch_size,\n","          'epochs': epochs,\n","          'mae': mae\n","      })\n","      ModelName=\"BiLSTM\"\n","      #mse, mse, r2, ModelName, stroke = model_evaluation(NextFrames, actual, LSTMpredictions, \"LSTM\", stroke, predictedFrameNumber)\n","      #data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Actual\", \"Values\": actual.tolist()})\n","      #data_to_save.append({\"ModelName\": ModelName, \"Stroke\": stroke, \"Type\": \"Predicted\", \"Values\": BiLSTMpredictions.tolist()})\n","      print(f'LSTM model Mean Absolute Error for next {NextFrames} frames: {mae}')\n","      # Convert results to a DataFrame\n","      results_df = pd.DataFrame(results)\n","\n","  csv_path = base_path + 'BiLSTM_grid_search_results.csv'\n","\n","  print(f\"CSV file saved at: {csv_path}\")\n","  results_df.to_csv(csv_path, index=False)\n","\n","  # Display the best configuration\n","  best_result = results_df.loc[results_df['mae'].idxmin()]\n","  print(\"\\nBest Configuration:\", best_result)\n","\n","  # Plotting MAE vs Hyperparameters\n","  plt.figure(figsize=(12, 6))\n","  for param in param_grid.keys():\n","      plt.plot(results_df[param], results_df['mae'], label=f'MAE vs {param}')\n","  plt.xlabel('Hyperparameter Values')\n","  plt.ylabel('Mean Absolute Error (MAE)')\n","  plt.title('Hyperparameter Tuning Results')\n","  plt.legend()\n","  plt.grid()\n","  plt.show()\n","###############################################\n","\n"]},{"cell_type":"code","execution_count":null,"id":"Jh6z9I2Icwn1","metadata":{"collapsed":true,"id":"Jh6z9I2Icwn1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"czWt-DiTnlyK","metadata":{"id":"czWt-DiTnlyK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"dmzTGae3nl1n","metadata":{"id":"dmzTGae3nl1n"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ZfZjEvhGrZDF","metadata":{"id":"ZfZjEvhGrZDF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"MLuXbENAnoao","metadata":{"id":"MLuXbENAnoao"},"outputs":[],"source":["# Define the LSTM model function\n","def lstm_modelgrid(NextFrames, data, X_train, y_train, X_test, y_test, train_size, sequence_length, n_features, lstm_units, learning_rate, batch_size, epochs):\n","    model = Sequential()\n","    model.add(LSTM(lstm_units, activation='relu', return_sequences=True, input_shape=(sequence_length, n_features)))\n","    model.add(LSTM(lstm_units, activation='relu'))\n","    model.add(Dense(NextFrames * n_features))\n","    model.add(Reshape((NextFrames, n_features)))\n","\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mean_squared_error')\n","\n","    # Train the model\n","    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n","\n","    # Evaluate the model on the test set\n","    predictions_scaled = model.predict(X_test[:1])  # Predict only the first set of test data\n","    predictions = scaler.inverse_transform(predictions_scaled.reshape(NextFrames, -1))\n","    actual = data[train_size:train_size + NextFrames]\n","    mae = mean_absolute_error(actual, predictions)\n","\n","    return model,actual,predictions, mae\n","\n","\n","# Function to create and train the BiLSTM model\n","def build_and_train_BiLSTM(units, batch_size, epochs, learning_rate, NextFrames, data, X_train, y_train, X_test, y_test, train_size):\n","    model = Sequential()\n","    model.add(Bidirectional(LSTM(units, activation='relu', return_sequences=True), input_shape=(X_train.shape[1], data.shape[1])))\n","    model.add(Bidirectional(LSTM(units, activation='relu')))\n","    model.add(Dense(NextFrames * data.shape[1]))\n","    model.add(Reshape((NextFrames, data.shape[1])))\n","\n","    # Compile with adjustable learning rate\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mean_squared_error')\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mse')\n","\n","    # Train the model\n","    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n","\n","    # Predict and evaluate\n","    predictions_scaled = model.predict(X_test[:1])\n","    predictions = scaler.inverse_transform(predictions_scaled.reshape(NextFrames, -1))\n","    actual = data[train_size:train_size+NextFrames]\n","    mae = mean_absolute_error(actual, predictions)\n","\n","    return mae\n","\n","############################################\n","def build_transformer_model(NextFrames, sequence_length, n_features, num_heads=4, key_dim=64, dense_units=64, dropout_rate=0.2):\n","    input_layer = Input(shape=(sequence_length, n_features))\n","\n","    # Multi-Head Attention Layer\n","    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(input_layer, input_layer)\n","    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n","\n","    # Fully Connected Layers\n","    transformer_output = Dense(dense_units, activation='relu')(attention_output)\n","    transformer_output = Dropout(dropout_rate)(transformer_output)\n","\n","    # Pooling to reduce dimensions to match NextFrames\n","    transformer_output = GlobalAveragePooling1D()(transformer_output)\n","\n","    # Predict the next frames\n","    output_layer = Dense(NextFrames * n_features)(transformer_output)\n","    output_layer = Reshape((NextFrames, n_features))(output_layer)\n","\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    model.compile(optimizer='adam', loss='mse')\n","    return model\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"gc-pmrKArW38","metadata":{"id":"gc-pmrKArW38"},"outputs":[],"source":["  '''\n","  # Compare predictions with actual data visually\n","  plt.figure(figsize=(12, 8))\n","\n","  for i in range(NextFrames):  # For each of the 3 predicted frames\n","      plt.subplot(NextFrames, 1, i+1)\n","      plt.plot(predictions[i], label='Predicted Frame {}'.format(train_size + i + 1))\n","      plt.plot(actual[i], label='Actual Frame {}'.format(train_size + i + 1))\n","      plt.legend()\n","      plt.title(f'Frame {train_size + i + 1} Comparison')\n","\n","  plt.tight_layout()\n","  plt.show()\n","  '''\n","\n"]},{"cell_type":"code","execution_count":null,"id":"XyXi9iYVfEsT","metadata":{"id":"XyXi9iYVfEsT"},"outputs":[],"source":["#Posdataset_numeric = Posdataset.drop(columns=['Unnamed: 0', 'Label'], axis=1)\n","#columns_list = Posdataset_numeric.columns.tolist()\n","#print(columns_list)"]},{"cell_type":"code","execution_count":null,"id":"d929f93a-b59b-4ab6-8fe2-ea08c9bbad43","metadata":{"id":"d929f93a-b59b-4ab6-8fe2-ea08c9bbad43"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"id":"85P8Z0GGlE5c","metadata":{"id":"85P8Z0GGlE5c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"tUzd_pb4lF24","metadata":{"id":"tUzd_pb4lF24"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3f68c08b-7b6c-4274-bd55-32a2f69c1870","metadata":{"id":"3f68c08b-7b6c-4274-bd55-32a2f69c1870"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"id":"L9SfT-wivsrb","metadata":{"id":"L9SfT-wivsrb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f90rIF2ifGnI","metadata":{"id":"f90rIF2ifGnI"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"yKczjn1OIXSH","metadata":{"id":"yKczjn1OIXSH"},"source":["**The END**"]},{"cell_type":"code","execution_count":null,"id":"bXJIFW36IR4a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":497,"status":"error","timestamp":1731657570116,"user":{"displayName":"Hafeez Siddiqui","userId":"18268930091638913773"},"user_tz":0},"id":"bXJIFW36IR4a","outputId":"e07e8b0b-7c20-4a85-ee68-78d9a8885cbb"},"outputs":[{"ename":"NameError","evalue":"name 'Posdataset' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-564990ee0766>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPosdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Assuming 'newdataset' is your DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcolumns_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Posdataset' is not defined"]}],"source":["import pandas as pd\n","import numpy as np\n","import plotly.graph_objects as go\n","\n","dataset = Posdataset.drop(columns=['Unnamed: 0', 'Label'], axis=1)\n","# Assuming 'newdataset' is your DataFrame\n","columns_list = dataset.columns.tolist()\n","\n","preframes = predictions_rescaled\n","\n","# Reshape frames to 2D (if it is a 3D array)\n","preframes = preframes.reshape(1, 132)\n","\n","# Now create the DataFrame\n","PredictedFrame = pd.DataFrame(preframes, columns=columns_list)\n","\n","orgframes = y_test_rescaled\n","orgframes = orgframes.reshape(1, 132)\n","OrignalFrame = pd.DataFrame(orgframes, columns=columns_list)\n","\n","# Load DataFrame with 33 joints (columns: 'x', 'y', 'z', 'visibility')\n","df1 = PredictedFrame  # Replace with your actual data\n","df2 = OrignalFrame  # Replace with your actual data\n","\n","def extract_coordinates(df):\n","    joints = 33  # Number of joints\n","    x, y, z = [], [], []\n","\n","    for i in range(joints):\n","        x.append(df1[f'F{i}_x'].values[0])  # Take the first row's x value\n","        y.append(df1[f'F{i}_y'].values[0])  # Take the first row's y value\n","        z.append(df1[f'F{i}_z'].values[0])  # Take the first row's z value\n","\n","    return np.array(x), np.array(y), np.array(z)\n","\n","# Extract coordinates for both poses\n","x1, y1, z1 = extract_coordinates(df1)\n","x2, y2, z2 = extract_coordinates(df2)\n","\n","# Define connections between joints based on MediaPipe PoseNet model\n","POSE_CONNECTIONS = [\n","    (0, 1), (1, 2), (2, 3), (3, 7),  # Right arm\n","    (0, 4), (4, 5), (5, 6), (6, 8),  # Left arm\n","    (9, 10), (11, 12),               # Shoulders\n","    (11, 13), (13, 15), (15, 17),    # Left leg\n","    (12, 14), (14, 16), (16, 18),    # Right leg\n","    (11, 23), (12, 24),              # Hips to shoulders\n","    (23, 24), (23, 25), (24, 26),    # Hips and upper legs\n","    (25, 27), (27, 29), (29, 31),    # Left lower leg\n","    (26, 28), (28, 30), (30, 32)     # Right lower leg\n","]\n","\n","# Function to plot the skeleton for a single pose in 3D\n","def plot_skeleton(x, y, z, connections, color='blue'):\n","    lines = []\n","    for joint1, joint2 in connections:\n","        lines.append(go.Scatter3d(\n","            x=[x[joint1], x[joint2]],\n","            y=[y[joint1], y[joint2]],\n","            z=[z[joint1], z[joint2]],\n","            mode='lines',\n","            line=dict(color=color, width=5)\n","        ))\n","    return lines\n","\n","# Create the 3D plot with both poses\n","fig = go.Figure()\n","\n","# Add skeleton for both poses (in red and blue)\n","fig.add_traces(plot_skeleton(x1, y1, z1, POSE_CONNECTIONS, color='red'))\n","fig.add_traces(plot_skeleton(x2, y2, z2, POSE_CONNECTIONS, color='blue'))\n","\n","# Set the layout for the 3D plot with increased size\n","fig.update_layout(\n","    title='3D Pose Skeleton Visualization',\n","    scene=dict(\n","        xaxis=dict(title='X-axis'),\n","        yaxis=dict(title='Y-axis'),\n","        zaxis=dict(title='Z-axis', range=[-1, 1]),  # Adjust range as needed\n","        aspectmode='cube'\n","    ),\n","    width=1200,  # Increased width\n","    height=800,  # Increased height\n","    showlegend=False\n",")\n","\n","# Display the plot\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"id":"JvAj-EyiN5HM","metadata":{"id":"JvAj-EyiN5HM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5y_oX84BN5Mh","metadata":{"id":"5y_oX84BN5Mh"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1UgWw2TxfEJQGTSQLiCE8fO8dL4VXavsr","timestamp":1731354692085}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}